{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: NLP Basics\n",
    "**Due date: November 26, 23:55**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: GloVe Embeddings\n",
    "In this section, you will load the traditional GloVe embeddings, and explore the basic properties of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe text files\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GloVe embeddings [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe word vectors into a dictionary\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_file_path = 'glove.6B/glove.6B.100d.txt'  # Replace with the path to your GloVe file\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "print(f'Loaded {len(glove_embeddings)} word vectors')\n",
    "print(glove_embeddings['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find closest words [5 pts]\n",
    "For a given word, you should output the similar words and their similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The words closest to 'man' are:\n",
      "woman with similarity of 0.8323\n",
      "boy with similarity of 0.7915\n",
      "one with similarity of 0.7789\n",
      "person with similarity of 0.7527\n",
      "another with similarity of 0.7522\n"
     ]
    }
   ],
   "source": [
    "# Function to find the closest word, and the corresponding similarity value\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def find_closest_word(word, embeddings_dict, top_n=1):\n",
    "    word_embedding = embeddings_dict[word]\n",
    "    closest_words = []\n",
    "    for w, emb in embeddings_dict.items():\n",
    "        if w != word:\n",
    "            closest_words.append((w, cosine_similarity(word_embedding, emb)))\n",
    "    closest_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return closest_words[:top_n]\n",
    "    \n",
    "chosen_word = 'man'\n",
    "closest_word = find_closest_word(chosen_word, glove_embeddings, top_n=5)\n",
    "print(f\"The words closest to '{chosen_word}' are:\")\n",
    "for word, similarity in closest_word:\n",
    "    print(f\"{word} with similarity of {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find new analogies [5 pts]\n",
    "In the lecture, we discussed how linear relationships exist in the embedding space (e.g. king - man + woman = queen). Please demonstrate an analogy that is not mentioned in the lecture. Feel free to alter the previous function if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rome', 0.8084002)]\n"
     ]
    }
   ],
   "source": [
    "word1 = \"paris\"\n",
    "word2 = \"france\"\n",
    "word3 = \"italy\"\n",
    "\n",
    "def find_analogy(word_a, word_b, word_c, embeddings_dict, top_n=1):\n",
    "    if word_a not in embeddings_dict or word_b not in embeddings_dict or word_c not in embeddings_dict:\n",
    "        print(f\"Error: One or more words not found in embeddings.\")\n",
    "        return []\n",
    "    \n",
    "    analogy_vector = (\n",
    "        np.array(embeddings_dict[word_a]) \n",
    "        - np.array(embeddings_dict[word_b]) \n",
    "        + np.array(embeddings_dict[word_c])\n",
    "    )\n",
    "    \n",
    "    similarities = []\n",
    "    for other_word, other_vector in embeddings_dict.items():\n",
    "        if other_word in {word_a, word_b, word_c}:\n",
    "            continue\n",
    "        similarity = cosine_similarity(analogy_vector, other_vector)\n",
    "        similarities.append((other_word, similarity))\n",
    "        \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "print(find_analogy(word1, word2, word3, glove_embeddings, top_n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: BPE tokenizer\n",
    "\n",
    "Byte Pair Encoding(BPE) is a subword tokenization technique that iteratively merges the most frequent adjacent byte pairs into subword units, creating a vocabulary that balances character-level granularity and whole-word tokens. This method is widely used in modern natural language processing to handle out-of-vocabulary words and optimize tokenization efficiency.\n",
    "\n",
    "Let's look at an example. Given a sample string \"banana bandana\", we can calculate the frequency of the character pairs: \n",
    "```python\n",
    "('a', 'n'): 4, ('n', 'a'): 3, ('b', 'a'): 2, ('a', ' '): 1, (' ','b'): 1, ('n', 'd'): 1, ('d', 'a'): 1\n",
    "```\n",
    "Which means that we can combine 'an' into a new token. In the next round, 'an' can now participate in the frequency count, giving:\n",
    "```python\n",
    "('b', 'an'): 2, ('an', 'a'): 2, ('an', 'an'): 1, ('a', ' '): 1, (' ','b'): 1, ('an', 'd'): 1, ('d', 'an'): 1\n",
    "```\n",
    "So we may get 'ban' as a new token. Similarly, 'ana' would be the most frequent pair in the next round. With three merges, we've added 'an', 'ban' and 'ana' into our vocabulary, and our string can now be converted to the following tokens:\n",
    "```python\n",
    "'ban', 'ana', ' ', 'ban','d' ,'ana'\n",
    "```\n",
    "So now we can use 6 tokens to represent the 14 characters.\n",
    "\n",
    "You may wonder how this is better than word-level tokenization. First of all, it is more robust in out-of-vocabulary scenarios. For example, though the word \"bandana\" does exist in the GloVe embedding (look it up if you're not convinced), something like \"banada\" does not. When using GloVe embeddings, encountering \"banada\" during training would result in the default \\<UNK\\> token. In contrast, a BPE tokenizer can still infer the word's meaning through its sub-word tokens. Secondly, sub-word tokens include prefixes and suffixes that allow the model to learn different variations of a single word more efficiently.\n",
    "\n",
    "In this section, you are required to implement a BPE tokenizer, and use one of the provided corpora to train it. You may train it on character level (starting with a vocabulary of all characters in the corpus) or byte level (starting with a vocabulary of all 256 possible byte values). You should verify that encoding and then decoding a sentence produces the original sentence. You may refer to (but not copy) the following implementations:\n",
    "1. The tiktoken library: https://github.com/openai/tiktoken/blob/main/tiktoken/_educational.py\n",
    "2. Kaparthy's minbpe repository: https://github.com/karpathy/minbpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation and Verification [15 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = []\n",
    "        self.merges = []\n",
    "    \n",
    "    def train(self, text: str, vocab_size: int):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on the given text.\n",
    "        \n",
    "        :param text: The input text to train the tokenizer.\n",
    "        :param vocab_size: The desired size of the final vocabulary.\n",
    "        \"\"\"\n",
    "        # Initialize the vocabulary with single characters\n",
    "        tokens = list(text)\n",
    "        self.vocab = list(set(tokens))\n",
    "        token_freq = defaultdict(int)\n",
    "        while(len(self.vocab) < vocab_size):\n",
    "            token_freq.clear()\n",
    "            for i in range(len(tokens) - 1):\n",
    "                token_freq[(tokens[i], tokens[i + 1])] += 1\n",
    "            \n",
    "            most_frequent = max(token_freq, key=token_freq.get)\n",
    "            new_token = ''.join(most_frequent)\n",
    "            self.merges.append(most_frequent)\n",
    "            \n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and tokens[i] == most_frequent[0] and tokens[i + 1] == most_frequent[1]:\n",
    "                    new_tokens.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "            self.vocab.append(new_token)\n",
    "        with open('vocab.txt', 'w', encoding='utf-8') as file:\n",
    "            for token in self.vocab:\n",
    "                file.write(token + '\\n')\n",
    "    \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Encode the input text using the learned BPE merges.\n",
    "        \n",
    "        :param text: The input text to encode.\n",
    "        :return: The list of tokens after encoding.\n",
    "        \"\"\"\n",
    "        tokens = list(text)\n",
    "        for merge in self.merges:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == merge:\n",
    "                    new_tokens.append(''.join(merge))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        return new_tokens\n",
    "    \n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode the input tokens using the learned BPE merges.\n",
    "        \n",
    "        :param tokens: The input tokens to decode.\n",
    "        :return: The decoded text.\n",
    "        \"\"\"\n",
    "        text = ''.join(tokens)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lor', 'em', ' ', 'i', 'p', 'su', 'm', ' d', 'ol', 'or', ' s', 'it ', 'am', 'e', 't, ', 'con', 'se', 'ct', 'et', 'ur', ' a', 'di', 'p', 'is', 'c', 'ing ', 'e', 'li', 't', '.']\n",
      "Voila!\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    train_text = file.read()\n",
    "    # print(train_text[:1000])\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(train_text, vocab_size=512)\n",
    "\n",
    "test_string = \"lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
    "print(tokenizer.encode(test_string))\n",
    "assert(tokenizer.decode(tokenizer.encode(test_string))==test_string)\n",
    "print(\"Voila!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: In your training process, which full words were the first to be merged into tokens? [5 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: on, is, you, to, and, the are the first several words to be merged into tokens. The common prepositions and pronouns are often the first to be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Text generation\n",
    "\n",
    "In this section, you will implement an RNN/LSTM model to generate sentences that mimic the style of the chosen corpus. You are free to use additional packages or modify the parameters of the provided function templates as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load necessary packages. Feel free to add ones that you need.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "from typing import Union\n",
    "import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess text [5 pts]\n",
    "You can choose a training corpus from the provided texts. Though the texts are much cleaner than random web crawls, you may still want to perform some preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speech', '1', '...', 'thank', 'you', 'so', 'much', '.', 'that', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "with open(\"dataset/trump_2016_speeches.txt\", \"r\", encoding=\"utf-8-sig\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Preprocess the text (tokenize, remove special characters, etc.)\n",
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess_text(text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary, and setup embedding matrix [5 pts]\n",
    "You may limit your vocabulary to words in the training corpus instead of using all 40k words in the GloVe embedding. Then, you may assign indices to each word and convert the embedding dictionary into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "def build_vocabulary(tokens):\n",
    "    word_to_idx = {\"<UNK>\": 0}\n",
    "    idx_to_word = {0: \"<UNK>\"}\n",
    "    idx = 1\n",
    "    for token in tokens:\n",
    "        if token not in word_to_idx:\n",
    "            word_to_idx[token] = idx\n",
    "            idx_to_word[idx] = token\n",
    "            idx += 1\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "# Load embeddings to matrix\n",
    "def load_glove_embeddings(glove_file, vocab):\n",
    "    embedding_matrix = None\n",
    "    embedding_dim = 0\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        embeddings_dict = {}\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_dict[word] = vector\n",
    "            embedding_dim = len(vector)\n",
    "        embedding_matrix = np.zeros((len(vocab), embedding_dim), dtype=np.float32)\n",
    "        for word, idx in vocab.items():\n",
    "            if word in embeddings_dict:\n",
    "                embedding_matrix[idx] = embeddings_dict[word]\n",
    "            else:\n",
    "                # all map to the <UNK> token with index 0\n",
    "                continue\n",
    "\n",
    "    return torch.tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the dataset [10 pts]\n",
    "The text generation task uses next word prediction as its objective. Think about how to construct your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, context_size, vocab):\n",
    "        \"\"\"\n",
    "        Construct a Dataset for next-word prediction.\n",
    "        \n",
    "        :param text: List of tokens (words) from the corpus.\n",
    "        :param context_size: Number of words to use as context for predicting the next word.\n",
    "        :param vocab: Dictionary mapping words to indices.\n",
    "        \"\"\"\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Preprocess the data to generate context-target pairs\n",
    "        self.data = []\n",
    "        for i in range(context_size, len(text)):\n",
    "            context = text[i - context_size:i]\n",
    "            target = text[i]\n",
    "            context_indices = [vocab[word] for word in context if word in vocab]\n",
    "            target_index = vocab[target]\n",
    "\n",
    "            # Ensure the context length matches context_size\n",
    "            if len(context_indices) == context_size:\n",
    "                self.data.append((context_indices, target_index))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single context-target pair.\n",
    "        \n",
    "        :param idx: Index of the data point.\n",
    "        :return: Tuple (context_tensor, target_tensor).\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        return context_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the RNN(LSTM) model [10 pts]\n",
    "You are allowed to use nn.LSTM in this section. Feel free to try different model architectures as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct your model\n",
    "class TextGenLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix):\n",
    "        \"\"\"\n",
    "        Initialize the TextGenLSTM model.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of word embeddings.\n",
    "        :param hidden_dim: Dimension of LSTM hidden states.\n",
    "        :param num_layers: Number of LSTM layers.\n",
    "        :param embedding_matrix: Pre-trained embedding matrix.\n",
    "        \"\"\"\n",
    "        super(TextGenLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x): # x: (batch_size, seq_length)\n",
    "        embeddings = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        out, _ = self.lstm(embeddings) # out: (batch_size, seq_length, D * hidden_dim)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class TextGenLSTMWithAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix):\n",
    "        \"\"\"\n",
    "        Initialize the TextGenLSTM with QKV Attention model.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of word embeddings.\n",
    "        :param hidden_dim: Dimension of LSTM hidden states.\n",
    "        :param num_layers: Number of LSTM layers.\n",
    "        :param embedding_matrix: Pre-trained embedding matrix.\n",
    "        \"\"\"\n",
    "        super(TextGenLSTMWithAttn, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # QKV attention layers\n",
    "        self.Wk = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wv = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wq = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, seq_length)\n",
    "        embeddings = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "        lstm_out, (hidden, _) = self.lstm(embeddings)\n",
    "\n",
    "        Q = self.Wq(hidden[-1])  # (batch_size, hidden_dim)\n",
    "\n",
    "        K = self.Wk(lstm_out)  # (batch_size, seq_length, hidden_dim)\n",
    "        V = self.Wv(lstm_out)  # (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(Q.unsqueeze(1), K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)  # (batch_size, 1, seq_length)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, 1, seq_length)\n",
    "\n",
    "        context = torch.matmul(attention_weights, V).squeeze(1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        output = self.fc(context)  # (batch_size, vocab_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a generate_text function [10 pts]\n",
    "Even with the same model, you can make it output totally different sentences. Try to make your model generate coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with your model\n",
    "\n",
    "def generate_text(model, start_sequence, num_words, vocab) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "\n",
    "    :param model: The trained text generation model.\n",
    "    :param start_sequence: A string representing the starting sequence.\n",
    "    :param num_words: Number of words to generate.\n",
    "    :param vocab: A dictionary with word-to-index and index-to-word mappings.\n",
    "                  Expected structure: {\"word2idx\": {}, \"idx2word\": {}}.\n",
    "    :param context_size: The context size used by the model.\n",
    "    :return: Generated text as a string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    word2idx = vocab[\"word2idx\"]\n",
    "    idx2word = vocab[\"idx2word\"]\n",
    "\n",
    "    # Tokenize the start sequence\n",
    "    start_sequence_tokens = preprocess_text(start_sequence)\n",
    "    input_indices = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in start_sequence_tokens]\n",
    "\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated_words = start_sequence_tokens[:]\n",
    "    new_words = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words):\n",
    "            output = model(input_tensor)  # (1, vocab_size)\n",
    "            predicted_idx = torch.argmax(output, dim=-1).item()\n",
    "            predicted_word = idx2word[predicted_idx]\n",
    "            generated_words.append(predicted_word)\n",
    "            new_words.append(predicted_word)\n",
    "            input_indices.append(predicted_idx)\n",
    "            input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(new_words).capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the training loop [10 pts]\n",
    "Now, you can finally train your model. During each epoch, remember to log appropriate stats, and let the model generate sentences to see how the training progresses. Since we're only using an RNN, there's no need to panic if you don't see GPT-like generation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, vocab, start_sequence, epochs, context_size: Union[int, None]):\n",
    "    \"\"\"\n",
    "    Train the model with logging and text generation.\n",
    "\n",
    "    :param model: The PyTorch model to train.\n",
    "    :param train_loader: DataLoader for training data.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param criterion: Loss function.\n",
    "    :param optimizer: Optimizer for updating model weights.\n",
    "    :param device: Device to run the model on ('cpu' or 'cuda').\n",
    "    :param vocab: Vocabulary containing word-to-index and index-to-word mappings.\n",
    "    :param start_sequence: String to initialize text generation.\n",
    "    :param epochs: Number of epochs to train.\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(epochs), total=epochs, desc=\"Epochs\"):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device).to(torch.long), targets.to(device).to(torch.long)\n",
    "            outputs = model(inputs).to(device)\n",
    "            loss = criterion(outputs, targets).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device).to(torch.long), targets.to(device).to(torch.long)\n",
    "                outputs = model(inputs).to(device)\n",
    "                loss = criterion(outputs, targets).to(device)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        # Log results to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "        })\n",
    "        \n",
    "    sample_text = generate_text(model, start_sequence, 10, vocab)\n",
    "    print(f\"Generated Example: {sample_text}\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"context_size\": 64,\n",
    "    \"glove_file\": \"glove.6B/glove.6B.300d.txt\",\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"epochs\": 15,\n",
    "    \"model\": \"lstm\", # or \"lstm_with_attn\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdora23333\u001b[0m (\u001b[33mdora23333-Peking University\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dora/Desktop/lab2/wandb/run-20241126_021450-em5ygs94</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dora23333-Peking%20University/llm/runs/em5ygs94' target=\"_blank\">lstm</a></strong> to <a href='https://wandb.ai/dora23333-Peking%20University/llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dora23333-Peking%20University/llm' target=\"_blank\">https://wandb.ai/dora23333-Peking%20University/llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dora23333-Peking%20University/llm/runs/em5ygs94' target=\"_blank\">https://wandb.ai/dora23333-Peking%20University/llm/runs/em5ygs94</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 15/15 [02:33<00:00, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Example: We have to be vigilant . we have to be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize training data\n",
    "word_2_idx, idx_2_word = build_vocabulary(tokens)\n",
    "dataset = TextDataset(tokens, config[\"context_size\"], word_2_idx)\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_matrix = load_glove_embeddings(config[\"glove_file\"], word_2_idx).to(device)\n",
    "vocab_size = len(word_2_idx)\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "# Initialize the data loader\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "# initialize the model\n",
    "if config[\"model\"] == \"lstm\":\n",
    "    model = TextGenLSTM(vocab_size, embedding_dim, config[\"hidden_dim\"], config[\"num_layers\"], embedding_matrix).to(device)\n",
    "elif config[\"model\"] == \"lstm_with_attn\":\n",
    "    model = TextGenLSTMWithAttn(vocab_size, embedding_dim, config[\"hidden_dim\"], config[\"num_layers\"], embedding_matrix).to(device)\n",
    "    \n",
    "# Initialize the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "    project=\"llm\",\n",
    "    name=f\"{config['model']}\",\n",
    "    config={\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"hidden_dim\": config[\"hidden_dim\"],\n",
    "        \"num_layers\": config[\"num_layers\"],\n",
    "        \"batch_size\": config[\"batch_size\"],\n",
    "        \"epochs\": config[\"epochs\"],\n",
    "        \"learning_rate\": config[\"learning_rate\"],\n",
    "        \"context_size\": config[\"context_size\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the lstm\n",
    "trained_lstm = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    vocab={\"word2idx\": word_2_idx, \"idx2word\": idx_2_word},\n",
    "    start_sequence=\"Trump is going to be the president of the United States in January next year. This has led to Chinese people worrying about the visa policy.\",\n",
    "    epochs=config[\"epochs\"],\n",
    "    context_size=config[\"context_size\"], \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
